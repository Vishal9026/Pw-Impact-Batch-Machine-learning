{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9568ef5e-4ee0-477f-b8bb-118b9137ef6f",
   "metadata": {},
   "source": [
    "# Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea2f048-40fb-452d-b8dd-9d550507d066",
   "metadata": {},
   "source": [
    "R-Squared (R² or the coefficient of determination) is a statistical measure in a regression model that determines the proportion of variance in the dependent variable that can be explained by the independent variable. In other words, r-squared shows how well the data fit the regression model (the goodness of fit)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db358d38-42d6-41bf-8897-b801d22d83fe",
   "metadata": {},
   "source": [
    "# Formula to calculate R-squared:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655c29d5-b31e-41a2-b1f9-1c561b6a312e",
   "metadata": {},
   "source": [
    "R\n",
    "2\n",
    "=\n",
    "1\n",
    "−\n",
    "sum squared regression (SSR)/\n",
    "total sum of squares (SST)\n",
    ",\n",
    "\n",
    "=\n",
    "1\n",
    "−\n",
    "∑\n",
    "(\n",
    "y\n",
    "i\n",
    "−\n",
    "^\n",
    "y\n",
    "i\n",
    ")\n",
    "2\n",
    "∑\n",
    "(\n",
    "y\n",
    "i\n",
    "−\n",
    "¯\n",
    "y\n",
    ")\n",
    "2\n",
    ".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242750fe-166e-485d-b03f-799074c0d18e",
   "metadata": {},
   "source": [
    "The sum squared regression is the sum of the residuals squared, and the total sum of squares is the sum of the distance the data is away from the mean all squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e21eb10-4bf0-47f4-a89a-c7e312e17101",
   "metadata": {},
   "source": [
    "The range of R squared is 0 to 1.If value closer to 1 it means better fit of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94abbc5-ff27-4259-9d7c-c388228be35a",
   "metadata": {},
   "source": [
    "It is important to note that R-squared does not indicate the causal relationship between the independent and dependent variables, nor does it indicate the accuracy or reliability of the model. Therefore, R-squared should be used in conjunction with other measures to evaluate the performance of a linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3654be53-7665-4436-bd2d-fea63d4c04d5",
   "metadata": {},
   "source": [
    "# Q2. Define adjusted R-squared and explain how it differs from the regular R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811f71b2-7149-4914-a8ec-437c63da4228",
   "metadata": {},
   "source": [
    "Adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. The adjusted R-squared increases when the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1697ab7e-c655-4dd9-a7ca-e01fc251353d",
   "metadata": {},
   "source": [
    "Adjusted R-squared differs from the regular R-squared are as following:\n",
    "    \n",
    "1-In this,it is increase when important feature are present other wise there is very high change.\n",
    "\n",
    "2-Adujected R-square < R-square\n",
    "\n",
    "3-Formula of Adjusted R-square is:\n",
    "    \n",
    "    Adjusted R-square = 1-(1-R 2)(N-1)/N-p-1\n",
    "\n",
    "Formula of Adjusted R-square is:\n",
    "    N=number of datapoint\n",
    "    p=Number of Independence feature\n",
    "    \n",
    "R 2 = 1 − sum squared regression (SSR)/ total sum of squares (SST) \n",
    "    \n",
    "Conculution:\n",
    "Many investors prefer adjusted R-squared because adjusted R-squared can provide a more precise view of the correlation by also taking into account how many independent variables are added to a particular model against which the stock index is measured.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fdf046-d70b-4a0d-9c82-ff7a6df217df",
   "metadata": {},
   "source": [
    "# Q3. When is it more appropriate to use adjusted R-squared?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f408a39-b685-4ce0-9908-da0ddb201ea3",
   "metadata": {},
   "source": [
    "Adjusted R-squared is generally more appropriate to use than regular R-squared when comparing the goodness of fit of linear regression models that have different numbers of independent variables.\n",
    "\n",
    "Regular R-squared can be misleading when comparing models with different numbers of independent variables because it always increases as more variables are added, even if the additional variables do not improve the fit of the model. This means that a model with a higher R-squared value may not necessarily be a better fit for the data.\n",
    "\n",
    "Adjusted R-squared, on the other hand, takes into account the number of independent variables in the model and penalizes the addition of unnecessary variables that do not improve the fit of the model. As a result, adjusted R-squared can give a more accurate measure of the goodness of fit of the model and is a better metric to use when comparing models with different numbers of independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbcb31f-e22c-435a-b4cd-a1eac036b8d8",
   "metadata": {},
   "source": [
    "# Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01fc3d9-0599-4c80-9744-d2b5f14b122c",
   "metadata": {},
   "source": [
    "# Mean absolute error(MAE)\n",
    "The Mean absolute error represents the average of the absolute difference between the actual and predicted values in the dataset. It measures the average of the residuals in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d92f8f-fd9e-48a1-ae4f-feb513136030",
   "metadata": {},
   "source": [
    "FORMULA:\n",
    "    \n",
    "    MAE = mean(abs(y_pred - y_actual))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9414a62b-c925-47cc-b776-b2717e38bda8",
   "metadata": {},
   "source": [
    "# Mean Squared Error(MSE)\n",
    "Mean Squared Error represents the average of the squared difference between the original and predicted values in the data set. It measures the variance of the residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad1b4b5-a75f-4c52-ba40-7e55be3a5616",
   "metadata": {},
   "source": [
    "FORMULA:\n",
    "    \n",
    "    MSE = mean((y_pred - y_actual)^2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54654151-bfd2-4f27-9410-f894f261cd96",
   "metadata": {},
   "source": [
    "# Root Mean Squared Error(RMSE)\n",
    "Root Mean Squared Error is the square root of Mean Squared error. It measures the standard deviation of residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00eece1-78b2-4f37-8274-4b511ba9695f",
   "metadata": {},
   "source": [
    "FORMULA:\n",
    "    \n",
    "    RMSE = sqrt(mean((y_pred - y_actual)^2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb18abf3-60f3-4c1a-88a9-dbdd29966099",
   "metadata": {},
   "source": [
    "All three metrics, Root Mean Squared Error (RMSE), Mean Squared Error (MSE), and Mean Absolute Error (MAE) are used to evaluate the performance of a regression model. A lower value of any of these metrics indicates a better fit of the model. Which metric to use depends on the specific problem at hand and the type of data involved. For example, Root Mean Squared Error (RMSE) is often used when the errors are normally distributed, while Mean Absolute Error (MAE) is preferred when there are outliers in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059fcd72-813d-4af5-a15b-d745ed467295",
   "metadata": {},
   "source": [
    "# Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885c2aaf-e1c4-4cff-933c-4dc69d3c06e5",
   "metadata": {},
   "source": [
    "#### Advantages of Root Mean Squared Error (RMSE):\n",
    "    \n",
    "1-Root Mean Squared Error (RMSE) gives higher weight to large errors, which can be useful when the cost of large errors is high.\n",
    "\n",
    "2-It is a popular metric in machine learning and regression analysis, and is widely used in competitions and benchmarks.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc208ec-2c07-4d09-8e58-6d7e51596caf",
   "metadata": {},
   "source": [
    "#### Disadvantages of Root Mean Squared Error (RMSE):\n",
    "    \n",
    "1-Root Mean Squared Error (RMSE) is sensitive to outliers, which can inflate the value of the metric.\n",
    "\n",
    "2-The square root operation in the formula can make the metric harder to interpret than other metrics, such as Mean Absolute Error (MAE).    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51407fe3-6398-4ced-a946-809161170fea",
   "metadata": {},
   "source": [
    "#### Advantages of Mean Squared Error (MSE):\n",
    "\n",
    "1-Mean Squared Error (MSE) gives equal weight to all errors, which can be useful when all errors are considered equally important.\n",
    "\n",
    "2-It is a popular metric in machine learning and regression analysis.\n",
    "\n",
    "#### Disadvantages of Mean Squared Error (MSE):\n",
    "\n",
    "1-Like Root Mean Squared Error (MSE), Mean Squared Error (MSE) is sensitive to outliers, which can inflate the value of the metric.\n",
    "\n",
    "2-It does not have an intuitive interpretation because the errors are squared.\n",
    "\n",
    "#### Advantages of Mean Absolute Error (MAE):\n",
    "\n",
    "1-Mean Absolute Error (MAE) is less sensitive to outliers than RMSE and MSE, which can make it more robust in the presence of outliers.\n",
    "\n",
    "2-It gives equal weight to all errors, which can be useful when all errors are considered equally important.\n",
    "\n",
    "3-Mean Absolute Error (MAE) is easy to interpret because it is expressed in the same units as the data.\n",
    "\n",
    "#### Disadvantages of Mean Absolute Error (MAE):\n",
    "\n",
    "1-Mean Absolute Error (MAE) gives less weight to large errors, which can be a disadvantage when the cost of large errors is high.\n",
    "\n",
    "2-It is not as widely used as Root Mean Squared Error (RMSE) & Mean Squared Error (MSE) in machine learning and regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21e3883-611d-4c20-9480-4d3babfd416d",
   "metadata": {},
   "source": [
    "# Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1ac984-6422-44c6-ac11-1a71440768c2",
   "metadata": {},
   "source": [
    "Lasso regularization, also known as L1 regularization, is a technique used in linear regression to prevent overfitting and improve the generalization performance of the model. It works by adding a penalty term to the objective function of the linear regression model, which encourages the coefficients of the model to be small or zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f288863a-2a3a-4d89-a0f5-715c9bbe39bc",
   "metadata": {},
   "source": [
    "Lasso regression is a regularization technique. It is used over regression methods for a more accurate prediction. This model uses shrinkage. Shrinkage is where data values are shrunk towards a central point as the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3788442a-6245-4d18-ae31-86c84a74d857",
   "metadata": {},
   "source": [
    "Similar to the lasso regression, ridge regression puts a similar constraint on the coefficients by introducing a penalty factor. However, while lasso regression takes the magnitude of the coefficients, ridge regression takes the square. Ridge regression is also referred to as L2 Regularization where as Lasso regulization is also referred as L1 regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4413b64-8469-4382-ac22-af9711112ecc",
   "metadata": {},
   "source": [
    "When deciding whether to use Lasso or Ridge regularization, it is important to consider the characteristics of the dataset and the goals of the analysis. Lasso regularization is more appropriate when there are many features in the dataset and it is suspected that only a few of them are important for the target variable. In this case, Lasso regularization can effectively perform feature selection and improve the interpretability of the model. Ridge regularization is more appropriate when there are many features that are all likely to contribute to the target variable, but the coefficients may be noisy or highly correlated. In this case, Ridge regularization can reduce the variance of the coefficients and improve the stability of the model.\n",
    "\n",
    "Overall, both Lasso and Ridge regularization are useful techniques for preventing overfitting in linear regression models, and the choice between them depends on the specific characteristics of the dataset and the goals of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed0cd31-1e7c-4775-a6b7-e916ef72defe",
   "metadata": {},
   "source": [
    "# Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b79724-a7a3-4eb4-9a90-b1a6837fac12",
   "metadata": {},
   "source": [
    "Overfitting impacts the accuracy of Machine Learning models. The model attempts to capture the data points that do not represent the accurate properties of data. These data points may be considered as noise. To avoid the occurrence of overfitting, we may use a method called regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8d2c6c-f422-454e-b8f5-8d4d73afcee9",
   "metadata": {},
   "source": [
    "For example, let's consider a linear regression problem where we want to predict the price of a house based on its size and number of bedrooms. We have a dataset of 1000 houses with their prices, sizes, and number of bedrooms. We can use this dataset to train a linear regression model to predict the price of a new house based on its size and number of bedrooms.\n",
    "\n",
    "However, if we include too many features in the model, such as the year the house was built, the style of the house, or the distance to the nearest park, the model may become too complex and overfit the training data. This means that the model may perform well on the training data, but it may perform poorly on new data that it has not seen before.\n",
    "\n",
    "To prevent overfitting in this scenario, we can use a regularized linear model, such as Ridge regression or Lasso regression. Ridge regression adds a penalty term to the objective function that is proportional to the square of the coefficients, while Lasso regression adds a penalty term that is proportional to the absolute value of the coefficients.\n",
    "\n",
    "Both Ridge and Lasso regression encourage the coefficients to be small, which reduces the complexity of the model and helps to prevent overfitting. Ridge regression is more effective when all the features are likely to be relevant, while Lasso regression is more effective when there are many irrelevant or redundant features in the dataset.\n",
    "\n",
    "By using a regularized linear model, we can find a balance between underfitting and overfitting the data, and improve the generalization performance of the model on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28548403-fa04-4ec9-ac95-b0c8b91f8bcf",
   "metadata": {},
   "source": [
    "# Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0a1834-018a-4c94-af23-dafaf451d798",
   "metadata": {},
   "source": [
    "While regularized linear models are useful for preventing overfitting and improving the generalization performance of linear regression models, they have some limitations that can make them less effective in certain situations. Here are some of the limitations of regularized linear models:\n",
    "\n",
    "Limited flexibility: Regularized linear models are linear models, which means they can only capture linear relationships between the features and the target variable. In situations where the relationship between the features and the target variable is highly non-linear, regularized linear models may not be the best choice.\n",
    "\n",
    "Feature selection limitations: While Lasso regularization is useful for feature selection, it can also be overly aggressive in some cases, setting some coefficients to zero even if they are important for the target variable. Ridge regularization, on the other hand, does not perform feature selection, which means that it may not be the best choice when there are many irrelevant or redundant features in the dataset.\n",
    "\n",
    "Hyperparameter tuning: Regularized linear models have hyperparameters that need to be tuned, such as the regularization parameter alpha in Ridge and Lasso regression. Tuning these hyperparameters can be time-consuming and requires cross-validation, which can add computational overhead.\n",
    "\n",
    "Limited interpretability : Regularized linear models can be less interpretable than non-regularized linear models because the coefficients are shrunk towards zero. This means that it may be more difficult to understand the exact relationship between the features and the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4111ea9-1e59-493d-9324-322370dc0f1a",
   "metadata": {},
   "source": [
    "# Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef0afb8-cccb-4d36-8880-2bec0c485168",
   "metadata": {},
   "source": [
    "The choice of which model is better depends on the specific context and priorities of the problem. Both RMSE and MAE are commonly used evaluation metrics in regression analysis, but they have different characteristics that can make one more appropriate than the other depending on the situation.\n",
    "\n",
    "RMSE emphasizes large errors because it squares the differences between the predicted and actual values. This means that RMSE is more sensitive to outliers than MAE. If the goal of the analysis is to minimize the impact of large errors on the overall performance of the model, then RMSE may be a more appropriate metric.\n",
    "\n",
    "MAE, on the other hand, is more robust to outliers because it takes the absolute value of the differences between the predicted and actual values. If the goal of the analysis is to minimize the impact of all errors, both small and large, then MAE may be a more appropriate metric.\n",
    "\n",
    "In the given scenario, Model B has a lower MAE than Model A, which means that it has a lower average absolute error between the predicted and actual values. This suggests that Model B may be better at making accurate predictions across the entire range of the target variable. However, it is important to note that this decision is based on the assumption that minimizing the impact of all errors, both small and large, is the main priority. If the problem requires more emphasis on reducing the impact of large errors, then RMSE would be a better metric.\n",
    "\n",
    "It is also worth noting that both RMSE and MAE have limitations. For example, they do not take into account the direction of the errors, and they may not capture the relative importance of different errors in the context of the problem. In some situations, it may be necessary to use additional evaluation metrics or consider other factors beyond the numerical performance metrics to make an informed decision about which model is better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14befb2-8ea1-466b-b4f9-d61cd332cee5",
   "metadata": {},
   "source": [
    "# Q10. You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d977bc93-3232-4f11-b935-af89786dee79",
   "metadata": {},
   "source": [
    "The choice of which regularized linear model is better depends on the specific context and priorities of the problem. Both Ridge and Lasso regularization are commonly used regularization methods in linear regression analysis, but they have different characteristics that can make one more appropriate than the other depending on the situation.\n",
    "\n",
    "Ridge regularization adds a penalty term to the loss function that is proportional to the square of the magnitude of the coefficients. This penalty term shrinks the coefficients towards zero, but it does not set them exactly to zero. This means that Ridge regularization can be effective at reducing the impact of irrelevant or redundant features in the dataset while still keeping all features in the model.\n",
    "\n",
    "Lasso regularization, on the other hand, adds a penalty term to the loss function that is proportional to the absolute value of the coefficients. This penalty term not only shrinks the coefficients towards zero but can also set some coefficients exactly to zero. This means that Lasso regularization can be effective at performing feature selection by setting the coefficients of irrelevant or redundant features to zero, resulting in a more interpretable model.\n",
    "\n",
    "In the given scenario, Model A uses Ridge regularization with a regularization parameter of 0.1, which means that it applies a moderate level of penalty to the coefficients to prevent overfitting. Model B uses Lasso regularization with a higher regularization parameter of 0.5, which means that it applies a higher level of penalty to the coefficients, potentially leading to a more sparse model.\n",
    "\n",
    "To determine which model is better, it is important to consider the specific priorities and goals of the analysis. If the main priority is to reduce overfitting while still keeping all relevant features in the model, then Model A with Ridge regularization may be a better choice. If the main priority is to perform feature selection and obtain a more interpretable model, then Model B with Lasso regularization may be a better choice.\n",
    "\n",
    "However, it is important to note that there are trade-offs and limitations to both Ridge and Lasso regularization. Ridge regularization does not perform feature selection and may result in a less interpretable model. Lasso regularization, on the other hand, can be overly aggressive in some cases, setting some coefficients to zero even if they are important for the target variable. Additionally, the choice of the regularization parameter for both methods can have a significant impact on the performance of the model, and it may require tuning through cross-validation or other methods to obtain the optimal value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a9fc82-afbb-4cb3-b929-88b007f27c21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062667cc-e1c1-463a-a1d2-7828ae5f457d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ec4efb-4635-4a8a-882b-25cf4a7be43a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f69517-0192-46d3-9458-314fc9e9d740",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f6370f-03fc-4e19-8d14-39632e310e5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9660a17-84f2-44e4-9a13-88f9d73e001a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9663b48d-1840-45d0-a9fe-201f03ab7c2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34c8f13-48c2-4271-abe3-6410e0e78c5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee85be8f-f9bc-4ad6-8ee9-a3b035b558b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7151513b-904d-48f4-9914-358fc89bf042",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8608ef-0a42-4eee-8835-6fcd30bd2705",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9606e422-a24c-40fb-a274-ea310dedf61c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04754546-cdd2-434a-8555-3bd3e5fa306a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
