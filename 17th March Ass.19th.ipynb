{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cacf32e4-4f61-4e8b-b3f6-b96f7b8cbe4c",
   "metadata": {},
   "source": [
    "# Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some algorithms that are not affected by missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbcc503-9400-4ebe-a264-843ce477ed40",
   "metadata": {},
   "source": [
    "Missing values occurs in dataset when some of the informations is not stored for a variable There are 3 mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d857ae10-7d75-478a-beda-b755b872f5f6",
   "metadata": {},
   "source": [
    "1-Missing Completely at Random,MCAR:\n",
    "If the data is MCAR, the missing values are randomly distributed throughout the dataset, and there is no systematic reason for why they are missing.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4f4b61-1a50-4e76-b994-6579f367d371",
   "metadata": {},
   "source": [
    "For example, in a survey about the prevalence of a certain disease, the missing data might be MCAR if the survey participants with missing values for certain questions were selected randomly and their missing responses are not related to their disease status or any other variables measured in the survey."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b897577-eef0-4490-80a7-ca275ba3f9ab",
   "metadata": {},
   "source": [
    "2-Missing at Random MAR:If the data is MAR, the missing values are systematically related to the observed data, but not to the missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152083be-bd8b-4b3e-9f23-96fc878dd389",
   "metadata": {},
   "source": [
    "Example:\n",
    "Income data: Suppose you are collecting income data from a group of people, but some participants choose not to report their income. If the decision to report or not report income is related to the participant's age or gender, but not to their income level, then the data is missing at random."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b611b5-a873-4f1d-a339-a3d797f9be9e",
   "metadata": {},
   "source": [
    "3.Missing data not at random (MNAR): if the data is MNAR, the missingness is not random and is dependent on unobserved or unmeasured factors that are associated with the missing values.\n",
    "\n",
    "For example, suppose you are collecting data on the income and job satisfaction of employees in a company. If employees who are less satisfied with their jobs are more likely to refuse to report their income, then the data is not missing at random. In this case, the missingness is dependent on job satisfaction, which is not directly observed or measured."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d573886f-a6a8-4e0f-9a6d-959c70757bab",
   "metadata": {},
   "source": [
    "It essential to handle missing values because when the data contain lot of missing value it not give me proper information about hole data and sometime absence of data contain lot of useful information so we can not ignore that type of data.So, we try to fill that data with help of different type of technique used. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3416e305-9387-4164-8efe-75f5db9896b4",
   "metadata": {},
   "source": [
    "# Some of the common algorithms that are not affected by missing values are:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250f789e-7c08-4043-bc6d-5042ee33cd12",
   "metadata": {},
   "source": [
    "1-Decision Trees\n",
    "\n",
    "2-Random Forests\n",
    "\n",
    "3-Gradient Boosting Machines\n",
    "\n",
    "4-K-Nearest Neighbors\n",
    "\n",
    "5-Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f49638-3037-4dc8-b280-588bf1066b8e",
   "metadata": {},
   "source": [
    "# Q2: List down techniques used to handle missing data. Give an example of each with python code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883cfb1b-7ba8-4a29-aa0a-063b1892b928",
   "metadata": {},
   "source": [
    "1-Deletion:\n",
    "Deletion involves removing the rows or columns that contain missing values from the dataset. This technique is only recommended when the amount of missing data is small relative to the size of the dataset, and the missing data is missing completely at random (MCAR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b98d392d-523a-4980-9b45-13553ce81c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "461976d2-0c5c-4c0a-937d-0ca674f9b717",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sns.load_dataset(\"titanic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64b3833c-99b6-434a-b324-f4fbef386f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n",
       "0         0       3    male  22.0      1      0   7.2500        S  Third   \n",
       "1         1       1  female  38.0      1      0  71.2833        C  First   \n",
       "2         1       3  female  26.0      0      0   7.9250        S  Third   \n",
       "3         1       1  female  35.0      1      0  53.1000        S  First   \n",
       "4         0       3    male  35.0      0      0   8.0500        S  Third   \n",
       "\n",
       "     who  adult_male deck  embark_town alive  alone  \n",
       "0    man        True  NaN  Southampton    no  False  \n",
       "1  woman       False    C    Cherbourg   yes  False  \n",
       "2  woman       False  NaN  Southampton   yes   True  \n",
       "3  woman       False    C  Southampton   yes  False  \n",
       "4    man        True  NaN  Southampton    no   True  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c2190b6-536a-4918-bfd7-640408df7157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "survived         0\n",
       "pclass           0\n",
       "sex              0\n",
       "age            177\n",
       "sibsp            0\n",
       "parch            0\n",
       "fare             0\n",
       "embarked         2\n",
       "class            0\n",
       "who              0\n",
       "adult_male       0\n",
       "deck           688\n",
       "embark_town      2\n",
       "alive            0\n",
       "alone            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47af42f1-dd01-4b9e-a49d-bf01ed263f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>E</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16.7000</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>child</td>\n",
       "      <td>False</td>\n",
       "      <td>G</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.5500</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>52.5542</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>D</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>B</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>83.1583</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>B</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>182 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n",
       "1           1       1  female  38.0      1      0  71.2833        C  First   \n",
       "3           1       1  female  35.0      1      0  53.1000        S  First   \n",
       "6           0       1    male  54.0      0      0  51.8625        S  First   \n",
       "10          1       3  female   4.0      1      1  16.7000        S  Third   \n",
       "11          1       1  female  58.0      0      0  26.5500        S  First   \n",
       "..        ...     ...     ...   ...    ...    ...      ...      ...    ...   \n",
       "871         1       1  female  47.0      1      1  52.5542        S  First   \n",
       "872         0       1    male  33.0      0      0   5.0000        S  First   \n",
       "879         1       1  female  56.0      0      1  83.1583        C  First   \n",
       "887         1       1  female  19.0      0      0  30.0000        S  First   \n",
       "889         1       1    male  26.0      0      0  30.0000        C  First   \n",
       "\n",
       "       who  adult_male deck  embark_town alive  alone  \n",
       "1    woman       False    C    Cherbourg   yes  False  \n",
       "3    woman       False    C  Southampton   yes  False  \n",
       "6      man        True    E  Southampton    no   True  \n",
       "10   child       False    G  Southampton   yes  False  \n",
       "11   woman       False    C  Southampton   yes   True  \n",
       "..     ...         ...  ...          ...   ...    ...  \n",
       "871  woman       False    D  Southampton   yes  False  \n",
       "872    man        True    B  Southampton    no   True  \n",
       "879  woman       False    C    Cherbourg   yes  False  \n",
       "887  woman       False    B  Southampton   yes   True  \n",
       "889    man        True    C    Cherbourg   yes   True  \n",
       "\n",
       "[182 rows x 15 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70cb8104-a53f-4421-a9c1-342f91956485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A    B   C\n",
       "0  1.0  5.0   9\n",
       "1  2.0  NaN  10\n",
       "2  NaN  NaN  11\n",
       "3  4.0  8.0  12"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4],\n",
    "                   'B': [5, np.nan, np.nan, 8],\n",
    "                   'C': [9, 10, 11, 12]})\n",
    "\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d70345c4-6083-40e5-81c6-84bf57794ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A    1\n",
       "B    2\n",
       "C    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af635b0c-9fc1-4e16-8356-34dcea113a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    C\n",
       "0   9\n",
       "1  10\n",
       "2  11\n",
       "3  12"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1ed39ebc-299b-4a5f-964f-4a2938627413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation Tecnique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabf37d2-3a3f-4e0c-8bd2-ad10665cd1dd",
   "metadata": {},
   "source": [
    "Imputation involves replacing the missing values with estimated values based on the available data. There are several methods of imputation, including mean imputation, median imputation, and mode imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "649da723-6043-47df-8975-9b845fc034b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.333333</td>\n",
       "      <td>6.5</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A    B   C\n",
       "0  1.000000  5.0   9\n",
       "1  2.000000  6.5  10\n",
       "2  2.333333  6.5  11\n",
       "3  4.000000  8.0  12"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4],\n",
    "                   'B': [5, np.nan, np.nan, 8],\n",
    "                   'C': [9, 10, 11, 12]})\n",
    "\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b5ddaa-8255-4e25-b7b7-da2426d1b3f7",
   "metadata": {},
   "source": [
    "# Q3 Explain the imbalanced data. What will happen if imbalanced data is not handled?\n",
    "Imbalanced data refers to a dataset where the number of instances in one class is significantly higher or lower than the number of instances in another class. In other words, the distribution of the target variable is not uniform.\n",
    "\n",
    "If imbalanced data is not handled, it can lead to several problems, including:\n",
    "\n",
    "1-Biased model performance: In the case of imbalanced data, a model may be biased towards the majority class because it has more data to learn from. This can result in poor performance on the minority class, which may be of greater importance in certain applications.\n",
    "\n",
    "2-False positives and false negatives: In imbalanced data, a model may predict the majority class with high accuracy, but perform poorly on the minority class. This can lead to a high number of false positives and false negatives.\n",
    "\n",
    "3-Overfitting: In the case of imbalanced data, a model may overfit to the majority class, resulting in poor generalization performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92ef254-cd78-466d-88ac-4b999c092372",
   "metadata": {},
   "source": [
    "# Q4. What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-sampling are required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d899883-7dae-435c-80d4-88b37a86ba37",
   "metadata": {},
   "source": [
    "Up-sampling refers to the process of increasing the number of instances in the minority class by randomly duplicating them. This can be done using techniques such as random oversampling or SMOTE (Synthetic Minority Over-sampling Technique).For example, suppose we have a dataset with 100 instances, out of which 90 belong to class A and 10 belong to class B. Since the dataset is imbalanced, we can up-sample the minority class B by randomly duplicating its instances, resulting in a new dataset with 100 instances, out of which 90 belong to class A and 20 belong to class B.\n",
    "\n",
    "Down-sampling refers to the process of decreasing the number of instances in the majority class by randomly removing them. This can be done using techniques such as random undersampling or Tomek links. For example, suppose we have a dataset with 100 instances, out of which 90 belong to class A and 10 belong to class B. Since the dataset is imbalanced, we can down-sample the majority class A by randomly removing some of its instances, resulting in a new dataset with 50 instances, out of which 45 belong to class A and 5 belong to class B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd3c524-0ac5-4c98-8a89-0ab2e0a55b2e",
   "metadata": {},
   "source": [
    "# Q5. What is data Augmentation? Explain SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a943e8-abbb-40ec-a527-a1a5d9a74146",
   "metadata": {},
   "source": [
    "Data augmentation is a set of techniques to artificially increase the amount of data by generating new data points from existing data. This includes making small changes to data or using deep learning models to generate new data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f63c34-f734-498c-a727-4213aead037f",
   "metadata": {},
   "source": [
    "Synthetic Minority Oversampling Technique (SMOTE) is a statistical technique for increasing the number of cases in your dataset in a balanced way. The component works by generating new instances from existing minority cases that you supply as input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07358330-4f00-4144-a53b-d1f0f0ff2ef0",
   "metadata": {},
   "source": [
    "# Q6. What are outliers in a dataset? Why is it essential to handle outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf00a4d-2701-482d-8ba2-6abc8393ffc2",
   "metadata": {},
   "source": [
    "An outlier is a value in a data set that is very different from the other values. That is, outliers are values unusually far from the middle.\n",
    "\n",
    "In most cases, outliers have influence on mean , but not on the median , or mode . Therefore, the outliers are important in their effect on the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317bc08e-fcb5-4aba-9e13-a607b3b93aa5",
   "metadata": {},
   "source": [
    "It is essential to handle outliers because they can have a significant impact on the performance of machine learning models. Outliers can affect the accuracy and generalization performance of a model, as they can bias the model towards the extreme values, leading to overfitting or underfitting. Outliers can also affect the results of statistical analysis, such as the mean, variance, and standard deviation, leading to inaccurate or misleading conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2f029e-7890-4eef-9b56-a748db9e58f1",
   "metadata": {},
   "source": [
    "# Q7. You are working on a project that requires analyzing customer data. However, you notice that some of the data is missing. What are some techniques you can use to handle the missing data in your analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a026c16-0f32-44ba-aacd-f922493aabcb",
   "metadata": {},
   "source": [
    "There are several techniques that can be used to handle missing data in customer data analysis:\n",
    "\n",
    "1-Deletion: One technique is to simply delete the rows or columns with missing data. This is only recommended if the missing data is a small percentage of the total dataset and if the missing data is completely at random. However, if the missing data is a large percentage of the dataset, this technique can result in a significant loss of information.\n",
    "\n",
    "2-Imputation: Another technique is to impute the missing values. This can be done using statistical methods such as mean imputation, median imputation, or mode imputation. Alternatively, more advanced techniques such as k-nearest neighbor imputation or multiple imputation can be used.\n",
    "\n",
    "3-Machine learning-based methods: Machine learning-based methods can also be used to handle missing data. For example, regression-based methods can be used to predict missing values based on other variables in the dataset.\n",
    "\n",
    "4-Domain knowledge: Domain knowledge can also be used to handle missing data. For example, if certain values are missing for a customer, but it is known that all customers with certain characteristics have the same value, then that value can be imputed for the missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b1c9e0-6494-4263-bf51-072b230cfe82",
   "metadata": {},
   "source": [
    "# Q8. You are working with a large dataset and find that a small percentage of the data is missing. What are some strategies you can use to determine if the missing data is missing at random or if there is a pattern to the missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669cacf0-930b-4538-936d-5902389cfad0",
   "metadata": {},
   "source": [
    "There are several strategies that can be used to determine if the missing data is missing at random or if there is a pattern to the missing data:\n",
    "\n",
    "1-Visualization: One strategy is to visualize the data using plots and graphs to see if there are any patterns or trends in the missing data. For example, a heatmap can be used to show which values are missing in the dataset.\n",
    "\n",
    "2-Summary statistics: Another strategy is to calculate summary statistics for the missing data and compare them to the summary statistics for the non-missing data. If there are significant differences between the two, this may suggest that the missing data is not missing at random.\n",
    "\n",
    "3-Imputation: Imputation can also be used to determine if the missing data is missing at random. If the imputed values are similar to the non-missing values, this may suggest that the missing data is missing at random. If the imputed values are significantly different, this may suggest that there is a pattern to the missing data.\n",
    "\n",
    "4-Statistical tests: Statistical tests can also be used to determine if the missing data is missing at random. For example, a chi-square test can be used to test whether the missing data is independent of other variables in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3583eb-170f-464d-953b-7f9eb5c1de5b",
   "metadata": {},
   "source": [
    "# Q9. Suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of interest, while a small percentage do. What are some strategies you can use to evaluate the performance of your machine learning model on this imbalanced dataset?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7bbd2e-32f8-4b7e-b641-49148e87aad3",
   "metadata": {},
   "source": [
    "1-Confusion Matrix: A confusion matrix is a table that summarizes the performance of a classifier on a dataset. It displays the number of true positives, false positives, true negatives, and false negatives. A confusion matrix can help to evaluate the performance of the model, especially when dealing with imbalanced datasets.\n",
    "\n",
    "2-Precision, Recall, and F1-score: Precision, recall, and F1-score are metrics that are commonly used to evaluate the performance of machine learning models on imbalanced datasets. Precision is the fraction of true positive predictions among all positive predictions, while recall is the fraction of true positive predictions among all actual positive instances. F1-score is the harmonic mean of precision and recall. These metrics are useful when evaluating the performance of models on imbalanced datasets because they take into account both the false positive and false negative rates.\n",
    "\n",
    "3-ROC Curve and AUC: ROC (Receiver Operating Characteristic) curve is a plot of the true positive rate against the false positive rate. AUC (Area Under the ROC Curve) is a metric that measures the area under the ROC curve. ROC curve and AUC can be used to evaluate the performance of a model on an imbalanced dataset. A high AUC value suggests that the model is performing well on the dataset.\n",
    "\n",
    "4-Resampling techniques: Resampling techniques such as oversampling the minority class or undersampling the majority class can also be used to balance the dataset. Once the dataset is balanced, standard metrics such as accuracy, precision, recall, F1-score, and ROC can be used to evaluate the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bae3dc-ba31-40d1-a9bc-05a2f2e76940",
   "metadata": {},
   "source": [
    "# Q10. When attempting to estimate customer satisfaction for a project, you discover that the dataset is unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to balance the dataset and down-sample the majority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fc5ebe-3cfd-4db2-a406-2e004d88511e",
   "metadata": {},
   "source": [
    "1-Random undersampling: This method involves randomly selecting a subset of observations from the majority class to match the size of the minority class. This can be done using techniques such as RandomUnderSampler from the imblearn library in Python.\n",
    "\n",
    "2-Synthetic minority oversampling technique (SMOTE): This method involves generating synthetic observations for the minority class to match the size of the majority class. SMOTE can be used in combination with random undersampling to balance the dataset. This can be done using techniques such as SMOTETomek from the imblearn library in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac7c0d4-2a64-48e9-82cd-58d0bb1f3034",
   "metadata": {},
   "source": [
    "# Q11. You discover that the dataset is unbalanced with a low percentage of occurrences while working on a project that requires you to estimate the occurrence of a rare event. What methods can you employ to balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5c56ee-0d5f-4898-b61f-f5f711dd42ac",
   "metadata": {},
   "source": [
    "1-Random oversampling: This method involves randomly duplicating observations from the minority class to match the size of the majority class. This can be done using techniques such as RandomOverSampler from the imblearn library in Python.\n",
    "\n",
    "2-Synthetic minority oversampling technique (SMOTE): This method involves generating synthetic observations for the minority class to match the size of the majority class. SMOTE can be used in combination with random oversampling to balance the dataset. This can be done using techniques such as SMOTE from the imblearn library in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d49622-2914-45e6-9493-033ee8e38cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7213dbfa-f83c-4c85-9a02-de6bd360f052",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a5b35e-93e1-451d-9748-507ad983aee6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd723ca0-6867-4e0c-b16c-3ef2e21c26e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472b6f84-723e-44c6-97b4-7f6500235b87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096efbc4-1de9-41c6-9ab0-0411bb7ae3a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e1adde-8290-4711-b19e-e1cf9c2209fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0513fa33-4f21-4f83-be6e-7c70f2ecb6c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f53977e-c30b-4065-bfa5-528f3326b45b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b57662f-0499-469c-9834-1b2dbd8bcd02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542b0224-b4e9-44c4-a105-633e3dc15ee1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d620fcec-a6e4-45b0-90cf-2e3b268ad59a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200a3592-87da-4619-80ce-141cafae4cdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
