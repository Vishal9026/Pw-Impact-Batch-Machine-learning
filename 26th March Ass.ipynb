{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d554db7d-6cca-4e29-b233-ab6b2ea2e0de",
   "metadata": {},
   "source": [
    "# Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b881fd8b-f93c-4d66-b1e0-911dc5d0e687",
   "metadata": {},
   "source": [
    "1-Simple Linear Regression:In Simple Linear Regression only one independence feature and one dependence feature means\n",
    "it has only one x and one y variable\n",
    "\n",
    "2-Multiple Linear Regression:In Multiple Linear Regression more than one independence feature and only one dependence feature means it has two or more x variable and one y variable.\n",
    "\n",
    "Simple Linear Regression\n",
    "\n",
    "For Example:When we predict weight based on height.\n",
    "\n",
    "Multiple Linear Regression\n",
    "\n",
    "For Example:When we predict revenue of the company on the basis on sale,profit,loss,employer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44e8dee-8000-4675-96d9-5771f7397267",
   "metadata": {},
   "source": [
    "# Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda180bb-e97d-49a6-859c-9497a0ee23c0",
   "metadata": {},
   "source": [
    "Linear regression is an analysis that assesses whether one or more predictor variables explain the dependent (criterion) variable.The regression has five key assumptions:\n",
    "    \n",
    "1-Linear relationship:Linear regression needs the relationship between the independent and dependent variables to be linear.  It is also important to check for outliers since linear regression is sensitive to outlier effects.  The linearity assumption can best be tested with scatter plots.\n",
    "\n",
    "2-Multivariate normality:The linear regression analysis requires all variables to be multivariate normal.  This assumption can best be checked with a histogram or a Q-Q-Plot. \n",
    "\n",
    "3-No or little multicollinearity:There is no perfect multicollinearity between the independent variables. This means that the independent variables should not be highly correlated with each other.\n",
    "\n",
    "4-No auto-correlation:Linear regression analysis requires that there is little or no autocorrelation in the data.  Autocorrelation occurs when the residuals are not independent from each other.  In other words when the value of y(x+1) is not independent from the value of y(x).\n",
    "\n",
    "5-Homoscedasticity:The last assumption of the linear regression analysis is homoscedasticity.  The scatter plot is good way to check whether the data are homoscedastic (meaning the residuals are equal across the regression line).   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78effe5e-bd5d-40d4-9ab8-b61fef927f66",
   "metadata": {},
   "source": [
    "To check whether these assumptions hold in a given dataset, various diagnostic tests can be performed. These tests include:\n",
    "\n",
    "1-Residual plots : plotting the residuals against the predicted values and independent variables to detect patterns that violate the assumptions of linearity, homoscedasticity, and normality.\n",
    "\n",
    "2-Normal probability plots: plotting the residuals against the expected normal distribution to check for normality.\n",
    "\n",
    "3-Outlier detection: identifying and examining the influence of outliers on the regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07379de5-04eb-4fbf-b919-0ae015d7d401",
   "metadata": {},
   "source": [
    "# Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e97a5f-08e2-41ee-ac64-efb674832b26",
   "metadata": {},
   "source": [
    "In a linear regression model, the slope and intercept represent the relationship between the dependent variable and independent variable(s). Specifically:\n",
    "\n",
    "The intercept (denoted by \"b0\") represents the predicted value of the dependent variable when all independent variables are equal to zero. It is the point where the regression line intersects with the y-axis.\n",
    "\n",
    "The slope (denoted by \"b1\") represents the change in the dependent variable for each one-unit increase in the independent variable. It is the steepness of the regression line and reflects the strength and direction of the relationship between the dependent variable and the independent variable.\n",
    "\n",
    "Here is an example using a real-world scenario:\n",
    "\n",
    "Suppose we want to understand the relationship between a person's height (independent variable) and their weight (dependent variable). We collect data on 50 individuals and use a simple linear regression model to analyze the data. The regression model provides the following output:\n",
    "\n",
    "Intercept (b0) = 50 Slope (b1) = 2\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "The intercept (50) represents the predicted weight of a person with a height of zero, which is not meaningful in this context. Therefore, we do not interpret the intercept in this example.\n",
    "\n",
    "The slope (2) represents the predicted increase in weight for each one-unit increase in height. So, for every one-inch increase in height, we predict an increase of two pounds in weight. This means that the relationship between height and weight is positive and strong."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9e0c6d-0ed0-4baf-bfa5-b52beaa891d1",
   "metadata": {},
   "source": [
    "# Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e3c39b-2c74-4f24-b875-1f69327cb71a",
   "metadata": {},
   "source": [
    "Gradient Descent (GD) is a popular optimization algorithm used in machine learning to minimize the cost function of a model. It works by iteratively adjusting the weights or parameters of the model in the direction of the negative gradient of the cost function, until the minimum of the cost function is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd895c12-2b51-403d-89f2-b3dd7fdbcc7d",
   "metadata": {},
   "source": [
    "Gradient descent is used in many machine learning algorithms, such as linear regression, logistic regression, neural networks, and support vector machines. By minimizing the cost function using gradient descent, we can find the optimal values of the parameters that maximize the accuracy of the model on the training data and generalize well to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afaee09-c037-47af-8b7b-5304cdb02930",
   "metadata": {},
   "source": [
    "# Q5.Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75f8d7b-1f1c-46a8-bd28-2f2eccebd65a",
   "metadata": {},
   "source": [
    "Multiple linear regression should be used when multiple independent variables determine the outcome of a single dependent variable. This is often the case when forecasting more complex relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c665c70-4f26-47cf-b9e3-b92d25e4e35d",
   "metadata": {},
   "source": [
    "A multiple regression formula has multiple slopes (one for each variable) and one y-intercept. It is interpreted the same as a simple linear regression formula except there are multiple variables that all impact the slope of the relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6039574-818b-441d-acd6-69b63f415e02",
   "metadata": {},
   "source": [
    "The multiple linear regression model differs from the simple linear regression model in several ways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6be84f-fd41-4e5a-acec-90686ed6b6d1",
   "metadata": {},
   "source": [
    "Firstly, the multiple linear regression model involves multiple independent variables instead of just one. This means that the model can capture more complex relationships between the dependent and independent variables, allowing us to control for the effects of other variables on the dependent variable.\n",
    "\n",
    "Secondly, the multiple linear regression model requires more parameters to estimate (i.e., b1, b2, ..., bp), which makes the model more complex and computationally intensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d29339e-1440-4705-b648-d7fec3d33154",
   "metadata": {},
   "source": [
    "# Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a115b30-e6e4-47fb-8f3a-aaa247c47213",
   "metadata": {},
   "source": [
    "Multicollinearity refers to a situation in which two or more independent variables in a multiple linear regression model are highly correlated with each other. This can cause problems in the model, such as making it difficult to interpret the coefficients of the variables and reducing the accuracy of the model's predictions.\n",
    "\n",
    "Detecting multicollinearity can be done using several methods, such as:\n",
    "\n",
    "Correlation matrix: A correlation matrix can be used to identify the correlation between the independent variables. If two or more variables have a high correlation coefficient (close to 1 or -1), it suggests that there may be multicollinearity in the model.\n",
    "\n",
    "Variance Inflation Factor (VIF): The VIF is a measure of the extent to which the variance of the estimated regression coefficients is inflated due to multicollinearity. A VIF value greater than 5 or 10 suggests that there may be multicollinearity in the model.\n",
    "\n",
    "Eigenvalues: The eigenvalues of the correlation matrix can be used to detect multicollinearity. If one or more eigenvalues are close to zero, it suggests that there may be multicollinearity in the model.\n",
    "\n",
    "Some of the ways we can address this issue are:\n",
    "\n",
    "Drop one or more of the highly correlated variables: One way to address multicollinearity is to drop one or more of the highly correlated variables from the model. This can help to reduce the correlation between the remaining variables and improve the accuracy of the model's predictions.\n",
    "\n",
    "Combine the highly correlated variables: Another way to address multicollinearity is to combine the highly correlated variables into a single variable. For example, if two variables measure the same concept but in different ways, we can create a composite variable that captures both concepts.\n",
    "\n",
    "Regularization: Regularization techniques, such as Ridge Regression and Lasso Regression, can be used to address multicollinearity by penalizing the magnitude of the regression coefficients. This can help to reduce the impact of the highly correlated variables and improve the accuracy of the model's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4762ed-811b-467a-a47b-6ec47f8876a9",
   "metadata": {},
   "source": [
    "# Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c1e234-4e46-46a1-a3b9-8acef6b08fe2",
   "metadata": {},
   "source": [
    "Polynomial regression is a form of Linear regression where only due to the Non-linear relationship between dependent and independent variables, we add some polynomial terms to linear regression to convert it into Polynomial regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d997e485-c852-4f45-88b2-90e71d65cd07",
   "metadata": {},
   "source": [
    "The main difference between linear regression and polynomial regression is the nature of the relationship between the dependent variable and the independent variable. In linear regression, the relationship is assumed to be linear, while in polynomial regression, the relationship can be non-linear and can take on a more complex shape. Polynomial regression allows for more flexibility in modeling the relationship between the variables and can provide a better fit to the data when the relationship is non-linear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21202c78-413a-49bc-a23c-716a2ffb92f1",
   "metadata": {},
   "source": [
    "# Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2268bd20-fad5-4b08-a979-a77df127b386",
   "metadata": {},
   "source": [
    "### Advantages of polynomial regression:\n",
    "\n",
    "Flexibility: Polynomial regression can model non-linear relationships between the dependent and independent variables, allowing for more flexibility in modeling complex relationships.\n",
    "\n",
    "Higher accuracy: Polynomial regression can provide a better fit to the data when the relationship between the variables is non-linear, leading to higher accuracy of predictions compared to linear regression.\n",
    "\n",
    "Interpretability: Polynomial regression can provide insight into the curvature of the relationship between the variables, which may be useful in certain applications.\n",
    "\n",
    "\n",
    "### Disadvantages of polynomial regression:\n",
    "\n",
    "Overfitting: Higher degree polynomial functions can easily overfit the data and may not generalize well to new data, leading to poor performance on test data.\n",
    "\n",
    "Extrapolation: Polynomial regression can be prone to errors when used for extrapolation, as the relationship between the variables may not be well-defined outside the range of the data used to train the model.\n",
    "\n",
    "Model selection: Selecting the degree of the polynomial can be challenging, as a higher degree polynomial may lead to overfitting, while a lower degree polynomial may not capture the true relationship between the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ea601e-0efe-437b-a6c9-5ce27b0f65c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e85106-181d-433a-b2fa-da9ce3f4af86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44128ced-877e-42eb-a3b1-1c9eee2996ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bbf8d8-f5b9-475d-ac47-5aecf2fcc18b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e685a94-e052-4932-9df2-c30d316a6362",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
